import { describe, it, expect } from 'vitest';
import { MarkdownLexer } from '../src/lexer';
import { TokenType } from '../src/types';

describe('MarkdownLexer é”™è¯¯å¤„ç†', () => {
  it('åº”è¯¥å¤„ç†æœªé¢„æœŸçš„å­—ç¬¦', () => {
    // æµ‹è¯•ä¸€äº› Unicode å­—ç¬¦
    const lexer = new MarkdownLexer('ðŸš€ emoji æµ‹è¯•');
    const tokens = lexer.tokenize();

    expect(tokens[0].type).toBe(TokenType.TEXT);
    expect(tokens[0].value).toBe('ðŸš€');
    expect(tokens[1].type).toBe(TokenType.WHITESPACE);
    expect(tokens[2].type).toBe(TokenType.TEXT);
    expect(tokens[2].value).toBe('emoji');
  });

  it('åº”è¯¥å¤„ç†ä¸å®Œæ•´çš„è¯­æ³•ç»“æž„', () => {
    const testCases = [
      '*', // å•ä¸ªæ˜Ÿå·ï¼ˆä¸æ˜¯ç²—ä½“ï¼‰
      '**', // æœªé—­åˆçš„ç²—ä½“
      '`', // å•ä¸ªåå¼•å·
      '[', // æœªé—­åˆçš„æ–¹æ‹¬å·
      '(', // æœªé—­åˆçš„åœ†æ‹¬å·
      '#'.repeat(10), // è¶…è¿‡6ä¸ª#
    ];

    testCases.forEach(testCase => {
      const lexer = new MarkdownLexer(testCase);
      const tokens = lexer.tokenize();

      // åº”è¯¥èƒ½æ­£å¸¸è§£æžï¼Œä¸æŠ›å‡ºé”™è¯¯
      expect(tokens.length).toBeGreaterThan(0);
      expect(tokens[tokens.length - 1].type).toBe(TokenType.EOF);
    });
  });

  it('åº”è¯¥å¤„ç†æžé•¿çš„è¾“å…¥', () => {
    const veryLongText = 'a'.repeat(10000);
    const lexer = new MarkdownLexer(veryLongText);
    const tokens = lexer.tokenize();

    expect(tokens[0].type).toBe(TokenType.TEXT);
    expect(tokens[0].value.length).toBe(10000);
    expect(tokens[1].type).toBe(TokenType.EOF);
  });
});
